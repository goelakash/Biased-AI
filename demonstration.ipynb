{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load all libraries\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = [line.strip() for line in open('data/positive-words.txt', encoding='latin-1').readlines() if line.strip() and not line.startswith(';')]\n",
    "negative_words = [line.strip() for line in open('data/negative-words.txt', encoding='latin-1').readlines() if line.strip() and not line.startswith(';')]\n",
    "\n",
    "# Please download the word embeddings from https://nlp.stanford.edu/projects/glove/\n",
    "# This file here is a symlink only\n",
    "\n",
    "def get_embeddings():\n",
    "    with open('data/glove.840B.300d.txt') as fp:\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for i, line in enumerate(fp):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            rows.append(np.array([float(x) for x in items[1:]], 'f'))\n",
    "        arr = np.vstack(rows)\n",
    "        print(arr.shape)\n",
    "        return pd.DataFrame(arr, index=labels, dtype='f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2196017, 300)\n"
     ]
    }
   ],
   "source": [
    "word_vectors = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced',\n",
       " '2-faces',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abort',\n",
       " 'aborted']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_vectors = word_vectors.loc[positive_vectors.index.intersection(positive_words)].dropna()\n",
    "negative_vectors = word_vectors.loc[negative_vectors.index.intersection(negative_words)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([positive_vectors, negative_vectors])\n",
    "targets = ([1 for word in positive_vectors.index] + [-1 for word in negative_vectors.index])\n",
    "labels = data.index\n",
    "train_data, test_data, train_target, test_target, train_labels, test_labels = train_test_split(data, targets, labels, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>talents</th>\n",
       "      <td>0.079738</td>\n",
       "      <td>0.225430</td>\n",
       "      <td>0.481470</td>\n",
       "      <td>-0.215010</td>\n",
       "      <td>-0.242330</td>\n",
       "      <td>0.242570</td>\n",
       "      <td>0.253780</td>\n",
       "      <td>0.295400</td>\n",
       "      <td>-0.611930</td>\n",
       "      <td>2.487300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511370</td>\n",
       "      <td>0.061727</td>\n",
       "      <td>-0.230910</td>\n",
       "      <td>0.267960</td>\n",
       "      <td>-0.066753</td>\n",
       "      <td>0.127250</td>\n",
       "      <td>0.288560</td>\n",
       "      <td>-0.149570</td>\n",
       "      <td>-0.231650</td>\n",
       "      <td>-0.091760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compliment</th>\n",
       "      <td>-0.482480</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>-0.266020</td>\n",
       "      <td>-0.285650</td>\n",
       "      <td>-0.059128</td>\n",
       "      <td>0.205160</td>\n",
       "      <td>0.142190</td>\n",
       "      <td>-0.319580</td>\n",
       "      <td>-0.545840</td>\n",
       "      <td>1.268800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218690</td>\n",
       "      <td>0.212340</td>\n",
       "      <td>-0.302480</td>\n",
       "      <td>0.171490</td>\n",
       "      <td>0.343380</td>\n",
       "      <td>-0.001801</td>\n",
       "      <td>0.173300</td>\n",
       "      <td>-0.157050</td>\n",
       "      <td>-0.097721</td>\n",
       "      <td>0.265140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authoritative</th>\n",
       "      <td>-0.363900</td>\n",
       "      <td>-0.260220</td>\n",
       "      <td>0.070223</td>\n",
       "      <td>0.621990</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>0.454250</td>\n",
       "      <td>0.376440</td>\n",
       "      <td>0.265910</td>\n",
       "      <td>0.406690</td>\n",
       "      <td>1.210200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.342780</td>\n",
       "      <td>-0.336240</td>\n",
       "      <td>-0.031502</td>\n",
       "      <td>0.217860</td>\n",
       "      <td>0.299480</td>\n",
       "      <td>-0.244840</td>\n",
       "      <td>0.438770</td>\n",
       "      <td>-0.374580</td>\n",
       "      <td>0.187750</td>\n",
       "      <td>-0.283670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>providence</th>\n",
       "      <td>0.042244</td>\n",
       "      <td>-0.433010</td>\n",
       "      <td>0.166920</td>\n",
       "      <td>-0.593430</td>\n",
       "      <td>0.109380</td>\n",
       "      <td>0.113960</td>\n",
       "      <td>-0.030350</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.418350</td>\n",
       "      <td>1.349000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173810</td>\n",
       "      <td>-0.027388</td>\n",
       "      <td>-0.387370</td>\n",
       "      <td>0.473360</td>\n",
       "      <td>-0.013823</td>\n",
       "      <td>-0.022718</td>\n",
       "      <td>0.668730</td>\n",
       "      <td>-0.116850</td>\n",
       "      <td>-0.290870</td>\n",
       "      <td>-0.217070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well-mannered</th>\n",
       "      <td>-0.530950</td>\n",
       "      <td>0.145910</td>\n",
       "      <td>-0.319220</td>\n",
       "      <td>-0.179750</td>\n",
       "      <td>0.314400</td>\n",
       "      <td>0.354560</td>\n",
       "      <td>0.048244</td>\n",
       "      <td>-0.335420</td>\n",
       "      <td>0.159160</td>\n",
       "      <td>0.084073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.406210</td>\n",
       "      <td>-0.358610</td>\n",
       "      <td>0.113150</td>\n",
       "      <td>0.566350</td>\n",
       "      <td>0.690270</td>\n",
       "      <td>0.448130</td>\n",
       "      <td>0.282260</td>\n",
       "      <td>-0.314580</td>\n",
       "      <td>-0.121280</td>\n",
       "      <td>0.069098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dismaying</th>\n",
       "      <td>0.263180</td>\n",
       "      <td>0.399460</td>\n",
       "      <td>0.919330</td>\n",
       "      <td>0.126840</td>\n",
       "      <td>-0.102450</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>-0.160810</td>\n",
       "      <td>0.511620</td>\n",
       "      <td>-0.128500</td>\n",
       "      <td>-0.128760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029655</td>\n",
       "      <td>0.152860</td>\n",
       "      <td>0.253370</td>\n",
       "      <td>0.086322</td>\n",
       "      <td>0.285280</td>\n",
       "      <td>0.090775</td>\n",
       "      <td>-0.109520</td>\n",
       "      <td>-0.015017</td>\n",
       "      <td>-0.500690</td>\n",
       "      <td>-0.366520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inefficiency</th>\n",
       "      <td>-1.105700</td>\n",
       "      <td>0.461170</td>\n",
       "      <td>0.839480</td>\n",
       "      <td>0.015746</td>\n",
       "      <td>-0.914220</td>\n",
       "      <td>0.027020</td>\n",
       "      <td>0.132380</td>\n",
       "      <td>-0.080266</td>\n",
       "      <td>-0.488800</td>\n",
       "      <td>0.949530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231600</td>\n",
       "      <td>0.177470</td>\n",
       "      <td>0.726290</td>\n",
       "      <td>-0.057088</td>\n",
       "      <td>-0.502810</td>\n",
       "      <td>0.448690</td>\n",
       "      <td>0.281280</td>\n",
       "      <td>0.564260</td>\n",
       "      <td>0.041583</td>\n",
       "      <td>0.049635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concession</th>\n",
       "      <td>0.344790</td>\n",
       "      <td>-0.487610</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>-0.004551</td>\n",
       "      <td>0.336730</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>-0.007839</td>\n",
       "      <td>-0.066173</td>\n",
       "      <td>0.037484</td>\n",
       "      <td>1.762600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420230</td>\n",
       "      <td>0.494760</td>\n",
       "      <td>0.236870</td>\n",
       "      <td>-0.029079</td>\n",
       "      <td>0.439590</td>\n",
       "      <td>0.019844</td>\n",
       "      <td>0.312140</td>\n",
       "      <td>-0.166460</td>\n",
       "      <td>0.319590</td>\n",
       "      <td>0.213910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anti-social</th>\n",
       "      <td>-0.203020</td>\n",
       "      <td>0.431070</td>\n",
       "      <td>-0.496370</td>\n",
       "      <td>0.275530</td>\n",
       "      <td>-0.277230</td>\n",
       "      <td>-0.365060</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>-0.127080</td>\n",
       "      <td>-0.101390</td>\n",
       "      <td>0.975830</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.378850</td>\n",
       "      <td>0.237820</td>\n",
       "      <td>0.488760</td>\n",
       "      <td>0.641170</td>\n",
       "      <td>0.410270</td>\n",
       "      <td>0.692650</td>\n",
       "      <td>0.094197</td>\n",
       "      <td>0.366780</td>\n",
       "      <td>-0.074061</td>\n",
       "      <td>0.064191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>splendid</th>\n",
       "      <td>0.237170</td>\n",
       "      <td>0.025011</td>\n",
       "      <td>-0.031719</td>\n",
       "      <td>-0.274420</td>\n",
       "      <td>-0.276110</td>\n",
       "      <td>0.355150</td>\n",
       "      <td>0.427570</td>\n",
       "      <td>0.124220</td>\n",
       "      <td>-0.528210</td>\n",
       "      <td>1.707400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.341590</td>\n",
       "      <td>0.006202</td>\n",
       "      <td>-0.372650</td>\n",
       "      <td>0.253760</td>\n",
       "      <td>-0.003798</td>\n",
       "      <td>0.334760</td>\n",
       "      <td>0.513250</td>\n",
       "      <td>0.141040</td>\n",
       "      <td>-0.027448</td>\n",
       "      <td>0.044597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1         2         3         4         5    \\\n",
       "talents        0.079738  0.225430  0.481470 -0.215010 -0.242330  0.242570   \n",
       "compliment    -0.482480  0.000423 -0.266020 -0.285650 -0.059128  0.205160   \n",
       "authoritative -0.363900 -0.260220  0.070223  0.621990  0.495600  0.454250   \n",
       "providence     0.042244 -0.433010  0.166920 -0.593430  0.109380  0.113960   \n",
       "well-mannered -0.530950  0.145910 -0.319220 -0.179750  0.314400  0.354560   \n",
       "dismaying      0.263180  0.399460  0.919330  0.126840 -0.102450  0.019753   \n",
       "inefficiency  -1.105700  0.461170  0.839480  0.015746 -0.914220  0.027020   \n",
       "concession     0.344790 -0.487610  0.999400 -0.004551  0.336730  0.258800   \n",
       "anti-social   -0.203020  0.431070 -0.496370  0.275530 -0.277230 -0.365060   \n",
       "splendid       0.237170  0.025011 -0.031719 -0.274420 -0.276110  0.355150   \n",
       "\n",
       "                    6         7         8         9      ...          290  \\\n",
       "talents        0.253780  0.295400 -0.611930  2.487300    ...    -0.511370   \n",
       "compliment     0.142190 -0.319580 -0.545840  1.268800    ...     0.218690   \n",
       "authoritative  0.376440  0.265910  0.406690  1.210200    ...    -0.342780   \n",
       "providence    -0.030350  0.717000  0.418350  1.349000    ...     0.173810   \n",
       "well-mannered  0.048244 -0.335420  0.159160  0.084073    ...    -0.406210   \n",
       "dismaying     -0.160810  0.511620 -0.128500 -0.128760    ...    -0.029655   \n",
       "inefficiency   0.132380 -0.080266 -0.488800  0.949530    ...     0.231600   \n",
       "concession    -0.007839 -0.066173  0.037484  1.762600    ...    -0.420230   \n",
       "anti-social    0.343200 -0.127080 -0.101390  0.975830    ...    -0.378850   \n",
       "splendid       0.427570  0.124220 -0.528210  1.707400    ...    -0.341590   \n",
       "\n",
       "                    291       292       293       294       295       296  \\\n",
       "talents        0.061727 -0.230910  0.267960 -0.066753  0.127250  0.288560   \n",
       "compliment     0.212340 -0.302480  0.171490  0.343380 -0.001801  0.173300   \n",
       "authoritative -0.336240 -0.031502  0.217860  0.299480 -0.244840  0.438770   \n",
       "providence    -0.027388 -0.387370  0.473360 -0.013823 -0.022718  0.668730   \n",
       "well-mannered -0.358610  0.113150  0.566350  0.690270  0.448130  0.282260   \n",
       "dismaying      0.152860  0.253370  0.086322  0.285280  0.090775 -0.109520   \n",
       "inefficiency   0.177470  0.726290 -0.057088 -0.502810  0.448690  0.281280   \n",
       "concession     0.494760  0.236870 -0.029079  0.439590  0.019844  0.312140   \n",
       "anti-social    0.237820  0.488760  0.641170  0.410270  0.692650  0.094197   \n",
       "splendid       0.006202 -0.372650  0.253760 -0.003798  0.334760  0.513250   \n",
       "\n",
       "                    297       298       299  \n",
       "talents       -0.149570 -0.231650 -0.091760  \n",
       "compliment    -0.157050 -0.097721  0.265140  \n",
       "authoritative -0.374580  0.187750 -0.283670  \n",
       "providence    -0.116850 -0.290870 -0.217070  \n",
       "well-mannered -0.314580 -0.121280  0.069098  \n",
       "dismaying     -0.015017 -0.500690 -0.366520  \n",
       "inefficiency   0.564260  0.041583  0.049635  \n",
       "concession    -0.166460  0.319590  0.213910  \n",
       "anti-social    0.366780 -0.074061  0.064191  \n",
       "splendid       0.141040 -0.027448  0.044597  \n",
       "\n",
       "[10 rows x 300 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=200, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SGDClassifier(max_iter=200, loss='log')\n",
    "model.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9622356495468278"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model.predict(test_data), test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
    "\n",
    "def get_sentiment(input_str):\n",
    "    embeddings = [word_vectors.loc[token.casefold()] for token in TOKEN_RE.findall(input_str)]\n",
    "    predictions = model.predict_log_proba(embeddings)\n",
    "    return (predictions[:, 1] - predictions[:, 0]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.233151308586075\n",
      "-9.688738095325608\n",
      "-16.99371687959816\n",
      "-1.9686040217022105\n"
     ]
    }
   ],
   "source": [
    "#Testing on trivial words\n",
    "print(get_sentiment(\"good\"))\n",
    "print(get_sentiment(\"bad\"))\n",
    "print(get_sentiment(\"ugly\"))\n",
    "print(get_sentiment(\"The good, the bad and the ugly\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4445063989124153\n",
      "-0.4437736356852131\n",
      "-2.4018470012600437\n"
     ]
    }
   ],
   "source": [
    "print(get_sentiment(\"My name is Emily\"))\n",
    "print(get_sentiment(\"My name is Yvonne\"))\n",
    "print(get_sentiment(\"My name is Shaniqua\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2332782816924801\n",
      "0.08885950109471236\n",
      "-0.4140725296787715\n"
     ]
    }
   ],
   "source": [
    "print(get_sentiment(\"My name is James\"))\n",
    "print(get_sentiment(\"My name is Mario\"))\n",
    "print(get_sentiment(\"My name is DeShawn\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
